### MODULE-IV (Short Answer Type Questions)

#### 1. Describe the difference between simple linear regression and multiple linear regression. [CO2]

**Simple Linear Regression**:
- Involves one dependent variable and one independent variable.
- Model equation: \( y = \beta_0 + \beta_1 x + \epsilon \), where \( y \) is the dependent variable, \( x \) is the independent variable, \( \beta_0 \) is the intercept, \( \beta_1 \) is the slope, and \( \epsilon \) is the error term.

**Multiple Linear Regression**:
- Involves one dependent variable and two or more independent variables.
- Model equation: \( y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon \), where \( y \) is the dependent variable, \( x_1, x_2, \ldots, x_n \) are the independent variables, \( \beta_0 \) is the intercept, \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients, and \( \epsilon \) is the error term.

#### 2. What is the trade-off between an underfitting and an overfitting model in regression modeling? [CO5]

- **Underfitting**:
  - Occurs when the model is too simple to capture the underlying patterns in the data.
  - Results in high bias and low variance.
  - Model performs poorly on both training and test data.

- **Overfitting**:
  - Occurs when the model is too complex and captures noise in the data.
  - Results in low bias and high variance.
  - Model performs well on training data but poorly on test data.

The trade-off involves finding a balance between bias and variance to achieve good generalization on new, unseen data.

#### 3. What is the least squares method, and how is it used to estimate the parameters of a linear regression model? [CO5]

The **least squares method** minimizes the sum of the squared differences between the observed values and the values predicted by the model. The objective is to find the line that best fits the data points.

- For a simple linear regression model \( y = \beta_0 + \beta_1 x + \epsilon \):
  - The sum of squared errors (SSE) is \( \sum (y_i - (\beta_0 + \beta_1 x_i))^2 \).
  - To find the optimal parameters (\( \beta_0 \) and \( \beta_1 \)), we take the partial derivatives of SSE with respect to \( \beta_0 \) and \( \beta_1 \), set them to zero, and solve the resulting equations.

#### 4. How is regression different from classification? [CO2]

- **Regression**:
  - Predicts a continuous output.
  - Example: Predicting house prices, temperature, or stock prices.
  - Model output is a real number.

- **Classification**:
  - Predicts a discrete label or category.
  - Example: Classifying emails as spam or not spam, diagnosing diseases, or recognizing handwriting.
  - Model output is a class label.

### MODULE-IV (Long Answer Type Questions)

#### 1. Explain the concept of linear regression and its application in predictive modeling. [CO4]

**Concept of Linear Regression**:
- Linear regression is a statistical method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation.
- **Simple Linear Regression**: Involves one independent variable.
  - Equation: \( y = \beta_0 + \beta_1 x + \epsilon \)
- **Multiple Linear Regression**: Involves multiple independent variables.
  - Equation: \( y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon \)

**Application in Predictive Modeling**:
- Used to predict future values based on historical data.
- Examples:
  - **Economics**: Predicting economic indicators like GDP.
  - **Healthcare**: Predicting patient outcomes based on medical history.
  - **Marketing**: Forecasting sales based on advertising spend and other factors.
- **Steps in Predictive Modeling**:
  - **Data Collection**: Gather relevant data.
  - **Data Preprocessing**: Clean and preprocess data.
  - **Model Training**: Fit the linear regression model to the training data.
  - **Model Evaluation**: Assess model performance using metrics like R-squared and RMSE.
  - **Prediction**: Use the model to make predictions on new data.

#### 2. Explain the concept of regularization in regression modeling. Discuss the difference between L1 regularization (Lasso) and L2 regularization (Ridge). [CO5]

**Concept of Regularization**:
- Regularization techniques are used to prevent overfitting by adding a penalty to the model complexity.
- The goal is to constrain or regularize the coefficient estimates towards zero to reduce the model's variance.

**L1 Regularization (Lasso)**:
- Adds a penalty equal to the absolute value of the magnitude of coefficients.
- **Penalty term**: \( \lambda \sum_{j=1}^{n} |\beta_j| \)
- **Objective function**: Minimize \( \sum (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{n} |\beta_j| \)
- **Effect**: Encourages sparsity, setting some coefficients to exactly zero, effectively selecting a simpler model.

**L2 Regularization (Ridge)**:
- Adds a penalty equal to the square of the magnitude of coefficients.
- **Penalty term**: \( \lambda \sum_{j=1}^{n} \beta_j^2 \)
- **Objective function**: Minimize \( \sum (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{n} \beta_j^2 \)
- **Effect**: Shrinks the coefficients towards zero but does not set them exactly to zero, reducing model complexity without feature selection.

**Difference**:
- Lasso can produce simpler models by setting some coefficients to zero (feature selection), while Ridge shrinks coefficients but keeps all variables in the model.
- Lasso is useful when we believe many features are irrelevant, and Ridge is useful when we expect all features to contribute to the outcome.

#### 3. What is the L1 penalty, and how is it used in LASSO regression? What is the elastic net regularization, and how does it combine L1 and L2 penalties? [CO4]

**L1 Penalty**:
- L1 penalty is the sum of the absolute values of the coefficients: \( \lambda \sum_{j=1}^{n} |\beta_j| \).
- Used in **LASSO (Least Absolute Shrinkage and Selection Operator)** regression.
- Objective function for LASSO: Minimize \( \sum (y_i - \hat{y_i})^2 + \lambda \sum_{j=1}^{n} |\beta_j| \).
- **Effect**: Encourages sparsity, setting some coefficients to zero, thus performing variable selection.

**Elastic Net Regularization**:
- Combines both L1 and L2 penalties.
- **Penalty term**: \( \alpha \lambda \sum_{j=1}^{n} |\beta_j| + (1 - \alpha) \lambda \sum_{j=1}^{n} \beta_j^2 \)
- **Objective function**: Minimize \( \sum (y_i - \hat{y_i})^2 + \alpha \lambda \sum_{j=1}^{n} |\beta_j| + (1 - \alpha) \lambda \sum_{j=1}^{n} \beta_j^2 \)
- **Effect**: Balances the benefits of both Lasso and Ridge regularizations. It can select groups of correlated variables (like Ridge) and enforce sparsity (like Lasso).
- **Parameter \( \alpha \)**: Controls the balance between L1 and L2 penalties (0 ≤ \( \alpha \) ≤ 1).
  - \( \alpha = 1 \): Equivalent to Lasso.
  - \( \alpha = 0 \): Equivalent to Ridge.
  - \( 0 < \alpha < 1 \): Combination of Lasso and Ridge.