### MODULE-I (Short Answer Type Questions)

#### 2. Illustrate with a simple example how supervised learning can be used in handling loan defaulters. [CO2]

**Example**: A bank wants to predict whether a loan applicant will default on a loan. The bank has historical data on past applicants, including features such as age, income, credit score, loan amount, and whether they defaulted on their loan.

1. **Data Collection**: Gather historical data with features (age, income, credit score, loan amount) and the target variable (defaulted or not).
2. **Training**: Use this data to train a supervised learning model (e.g., logistic regression or decision tree). The model learns the relationship between the features and the likelihood of default.
3. **Prediction**: For a new applicant, the model uses the learned relationship to predict whether the applicant will default.

#### 3. Explain the concept of the gradient in gradient descent. [CO4]

**Concept**: In gradient descent, the gradient is a vector that represents the direction and rate of fastest increase of a function. For a cost function \( J(\theta) \), the gradient \( \nabla J(\theta) \) indicates how to adjust the parameters \( \theta \) to decrease the cost. Gradient descent iteratively updates \( \theta \) in the opposite direction of the gradient to minimize the cost function.

#### 4. What is the difference between overfitting and underfitting? [CO4]

**Overfitting**: A model is overfitting if it captures the noise and random fluctuations in the training data instead of the underlying pattern. It performs well on the training data but poorly on unseen data.

**Underfitting**: A model is underfitting if it is too simple to capture the underlying pattern in the data. It performs poorly on both the training data and unseen data.

**Difference**: Overfitting models are too complex and fit the training data too closely, while underfitting models are too simple and fail to capture the data's complexity.

#### 5. What is cross-validation, and how is it used in machine learning? [CO4]

**Cross-Validation**: Cross-validation is a technique for assessing how a machine learning model will generalize to an independent data set. It involves partitioning the data into training and testing sets multiple times to evaluate the model's performance.

**Usage**:
1. **K-Fold Cross-Validation**: The data is divided into \( k \) subsets (folds). The model is trained \( k \) times, each time using \( k-1 \) folds for training and 1 fold for testing. The results are averaged to provide a robust performance estimate.
2. **Benefits**: It helps in assessing the model's ability to generalize and avoid overfitting.

#### 6. What is unsupervised learning, and how does it work? [CO3]

**Unsupervised Learning**: Unsupervised learning involves training a model on data without labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data.

**How it Works**: Algorithms like clustering (e.g., k-means) or dimensionality reduction (e.g., PCA) are used to analyze the data and uncover patterns such as groupings or principal components without using predefined labels.

#### 7. What is supervised learning, and how does it work? [CO3]

**Supervised Learning**: Supervised learning involves training a model on a labeled dataset, where each training example has an input and an associated output label. The goal is to learn a mapping from inputs to outputs.

**How it Works**: Algorithms like linear regression, decision trees, or neural networks are used to create a function that maps input features to the target variable. The model is trained using labeled data and evaluated using a separate validation set to ensure it generalizes well to unseen data.

#### 8. What is reinforcement learning, and how does it work? [CO4]

**Reinforcement Learning (RL)**: RL is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.

**How it Works**: 
- The agent interacts with the environment by taking actions.
- The environment provides feedback in the form of rewards or penalties.
- The agent learns to optimize its actions based on the feedback to achieve the highest cumulative reward.

### MODULE-I (Long Answer Type Questions)

#### A1. What is machine learning, and how does it differ from traditional programming approaches? [CO1]

**Machine Learning**: Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions based on new data.

**Differences from Traditional Programming**:
- **Traditional Programming**: Involves explicitly coding instructions for every task. The logic and rules are manually crafted by programmers.
- **Machine Learning**: Involves creating models that learn from data. The system discovers patterns and rules from the data automatically.

#### A2. Explain how unsupervised learning differs from supervised learning and provide examples of unsupervised learning tasks. [CO2]

**Differences**:
- **Supervised Learning**: Uses labeled data to train models. The goal is to predict the output for new inputs.
- **Unsupervised Learning**: Uses unlabeled data to find hidden patterns or structures. There are no predefined labels.

**Examples of Unsupervised Learning Tasks**:
- **Clustering**: Grouping similar data points together (e.g., customer segmentation).
- **Dimensionality Reduction**: Reducing the number of features in a dataset while preserving its structure (e.g., PCA for data visualization).

#### A3. What is reinforcement learning, and how does it differ from supervised and unsupervised learning? Provide examples of reinforcement learning applications. [CO1]

**Reinforcement Learning (RL)**: RL involves learning optimal actions through trial and error by interacting with an environment and receiving rewards or penalties.

**Differences**:
- **Supervised Learning**: Learning from labeled data with a clear target outcome.
- **Unsupervised Learning**: Learning from unlabeled data to find patterns.
- **Reinforcement Learning**: Learning from interactions with an environment to maximize cumulative reward.

**Examples of RL Applications**:
- **Game Playing**: AlphaGo learning to play and win at Go.
- **Robotics**: Robots learning to navigate and perform tasks autonomously.
- **Autonomous Vehicles**: Self-driving cars learning to make driving decisions.

#### A4. Discuss the applications of unsupervised learning in various fields, such as recommendation systems, anomaly detection, and data exploration. [CO6]

**Applications**:
- **Recommendation Systems**: Clustering users based on behavior to recommend products or content (e.g., Netflix recommending shows).
- **Anomaly Detection**: Identifying unusual patterns that do not conform to expected behavior (e.g., fraud detection in financial transactions).
- **Data Exploration**: Discovering hidden structures in data to generate insights (e.g., market basket analysis in retail).

#### A5. Explain how overfitting occurs in machine learning models, and provide examples to illustrate the concept. [COS]

**Overfitting**: Occurs when a model learns the noise and details in the training data to the extent that it negatively impacts the model's performance on new data.

**Examples**:
- **Complex Model**: A polynomial regression model with too many degrees of freedom fitting the training data perfectly but performing poorly on test data.
- **Decision Trees**: A decision tree that splits too deeply, capturing noise and outliers rather than general patterns.

#### A6. Discuss the role of regularization in mitigating overfitting. How does regularization prevent models from fitting the noise in the training data? [CO3]

**Role of Regularization**: Regularization techniques add a penalty to the loss function to discourage the model from becoming too complex and overfitting the training data.

**How it Prevents Overfitting**:
- **L1 Regularization (Lasso)**: Adds a penalty equal to the absolute value of the coefficients, leading to sparse models with fewer features.
- **L2 Regularization (Ridge)**: Adds a penalty equal to the square of the coefficients, shrinking the coefficients and reducing model complexity.

By penalizing large coefficients, regularization forces the model to focus on the most important features, reducing the likelihood of fitting noise in the training data.