### MODULE - III (Short Answer Type Questions)

#### 1. How is the Fowlkes-Mallows index used to evaluate clustering? [CO4]

**Fowlkes-Mallows Index**: The Fowlkes-Mallows index (FMI) is a measure used to evaluate the quality of clustering results by comparing the clusters obtained by a clustering algorithm to the true clusters (ground truth). It considers the precision and recall of the clustering results.
  where:
  - \( TP \) (True Positives): The number of pairs of points that are in the same cluster in both the predicted and true clustering.
  - \( FP \) (False Positives): The number of pairs of points that are in the same cluster in the predicted clustering but not in the true clustering.
  - \( FN \) (False Negatives): The number of pairs of points that are in the same cluster in the true clustering but not in the predicted clustering.

- **Purpose**: FMI ranges from 0 to 1, where 1 indicates perfect clustering and 0 indicates poor clustering. It helps in evaluating how well the clustering algorithm has performed in terms of both precision and recall.

#### 2. What are the advantages and disadvantages of the k-means clustering algorithm? [CO4]

**Advantages**:
1. **Simplicity**: K-means is easy to understand and implement.
2. **Efficiency**: It works well with large datasets and is computationally efficient.
3. **Scalability**: K-means scales well to large datasets.
4. **Speed**: Converges quickly compared to other clustering algorithms.

**Disadvantages**:
1. **Fixed Number of Clusters**: Requires the number of clusters (\( k \)) to be specified in advance.
2. **Sensitivity to Initial Centroids**: The result can vary significantly based on the initial placement of centroids.
3. **Not Suitable for All Data Shapes**: Assumes clusters are spherical and of similar size, which may not be true for all datasets.
4. **Sensitivity to Outliers**: Outliers can significantly affect the resulting clusters.

#### 3. Give an example of an application for the k-means clustering algorithm. Explain in brief. [CO6]

**Example**: **Customer Segmentation in Marketing**

**Application**: Companies use k-means clustering to segment their customers into different groups based on purchasing behavior, demographics, and other relevant features.

**Explanation**:
1. **Data Collection**: Gather customer data, including features such as age, income, purchase history, and browsing behavior.
2. **Preprocessing**: Normalize the data to ensure all features contribute equally to the distance metric.
3. **Clustering**: Apply the k-means algorithm with an appropriate \( k \) (number of customer segments).
4. **Interpretation**: Analyze the resulting clusters to identify distinct customer segments. For example, one cluster might represent young, high-income customers who purchase frequently, while another cluster might represent older, low-income customers with infrequent purchases.
5. **Action**: Use the insights gained to tailor marketing strategies, create targeted promotions, and improve customer satisfaction.

#### 4. Describe various types of regression. [CO3]

**Types of Regression**:
1. **Linear Regression**: Models the relationship between a dependent variable and one or more independent variables using a linear equation
2. **Logistic Regression**: Used for binary classification problems. It models the probability that a given input belongs to a particular class.
3. **Polynomial Regression**: Extends linear regression by allowing the relationship between the dependent and independent variables to be modeled as an \( n \)-th degree polynomial.
4. **Ridge Regression (L2 Regularization)**: Linear regression with L2 regularization, which adds a penalty term proportional to the sum of the squares of the coefficients to prevent overfitting.
5. **Lasso Regression (L1 Regularization)**: Linear regression with L1 regularization, which adds a penalty term proportional to the sum of the absolute values of the coefficients to enforce sparsity.
6. **Elastic Net Regression**: Combines L1 and L2 regularization, balancing the benefits of both ridge and lasso regression.
  

#### 5. What is mutual information, and how is it used to evaluate clustering algorithms? [CO1]

**Mutual Information (MI)**: MI measures the amount of information obtained about one random variable through another random variable. It quantifies the dependency between two variables.

**Use in Clustering Evaluation**:
- **Application**: MI is used to compare the similarity between the clustering results and the ground truth labels.
- **Normalized Mutual Information (NMI)**: A normalized version of MI that adjusts for the chance level of mutual information, making it easier to compare across different datasets and clustering algorithms.
**Purpose**: A higher MI or NMI indicates a better alignment between the clustering results and the true labels, thus evaluating the quality of the clustering algorithm.

#### 6. What is the purpose of clustering evaluation? [CO2]

**Purpose of Clustering Evaluation**:
1. **Assess Performance**: To determine how well the clustering algorithm has grouped similar data points together and separated different data points.
2. **Compare Algorithms**: To compare the effectiveness of different clustering algorithms on a given dataset.
3. **Parameter Tuning**: To help in selecting the best parameters (e.g., number of clusters) for the clustering algorithm.
4. **Validate Results**: To validate the results of clustering by comparing them with ground truth labels or using internal metrics.
5. **Ensure Practical Relevance**: To ensure that the clusters are meaningful and useful for the specific application, such as customer segmentation or anomaly detection.


### MODULE - III (Long Answer Type Questions)

#### 4. What are the features of the KNN algorithm? What are the advantages and disadvantages of the KNN algorithm? [CO4]

**Features of the K-Nearest Neighbors (KNN) Algorithm**:
1. **Instance-Based Learning**: KNN is a type of instance-based learning or lazy learning where the function is approximated locally, and all computations are deferred until classification.
2. **Non-Parametric**: KNN is non-parametric, meaning it does not make any assumptions about the underlying data distribution.
3. **Distance Metric**: It relies on a distance metric (e.g., Euclidean distance) to find the closest neighbors.
4. **Simple and Intuitive**: KNN is easy to understand and implement, making it a popular choice for simple classification tasks.

**Advantages of KNN**:
1. **Simplicity**: Easy to understand and implement.
2. **No Training Phase**: Since KNN is a lazy learner, there is no explicit training phase.
3. **Versatility**: Can be used for both classification and regression tasks.
4. **Adaptability**: Can handle multi-class classification problems.

**Disadvantages of KNN**:
1. **Computational Cost**: The algorithm can be computationally expensive during prediction, especially with large datasets, as it requires calculating the distance to all training examples.
2. **Storage Cost**: Requires storing all training data, which can be memory-intensive.
3. **Sensitivity to Noise**: KNN can be sensitive to irrelevant or noisy features, which can affect accuracy.
4. **Choice of \( k \)**: The performance of KNN is highly dependent on the choice of \( k \) (number of neighbors). Choosing an appropriate \( k \) can be challenging and often requires cross-validation.

