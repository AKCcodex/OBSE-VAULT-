### MODULE-V

#### Short Answer Type Questions

**1. What is logistic regression, and how is it used in binary classification? [CO5]**

**Logistic Regression**:
- Logistic regression is a statistical method used for binary classification tasks, where the outcome variable is categorical with two possible values (e.g., yes/no, 0/1, true/false).
- It models the probability that a given input point belongs to a certain class.
- The model uses a logistic function (sigmoid function) to map any real-valued number into the interval [0, 1].



**2. Explain the concept of the support vector machine (SVM) algorithm in classification. [CO3]**

**Support Vector Machine (SVM)**:
- SVM is a supervised learning algorithm used for classification tasks.
- It works by finding the hyperplane that best separates the data into classes.
- The best hyperplane is the one that maximizes the margin, which is the distance between the hyperplane and the nearest data points from both classes (called support vectors).
- SVM can be extended to non-linear boundaries using kernel functions (e.g., polynomial, radial basis function).

**3. Why is it called "logistic regression" if it is used for classification? [CO1]**

Despite being a classification method, it is called "logistic regression" because:
- It uses the logistic (sigmoid) function to model the probability of the dependent variable.
- The term "regression" refers to its origins in the statistical method of predicting an outcome based on one or more predictors.
- Unlike linear regression, which predicts continuous outcomes, logistic regression predicts probabilities of categorical outcomes.

**4. What is a decision tree? [CO3]**

**Decision Tree**:
- A decision tree is a flowchart-like tree structure used for classification and regression tasks.
- Internal nodes represent features (attributes), branches represent decision rules, and each leaf node represents an outcome (class label or value).
- The tree is constructed by recursively splitting the data into subsets based on the feature that results in the most significant information gain (or other criteria like Gini impurity).
- Decision trees are easy to interpret and visualize, but they can be prone to overfitting.

### Long Answer Type Questions

**1. Why is SVM more accurate than logistic regression? What are the real-world applications of SVM? [CO2]**

**Why SVM can be more accurate**:
- **Margin Maximization**: SVM aims to find the hyperplane that maximizes the margin, leading to better generalization on unseen data.
- **Robust to Outliers**: By focusing on support vectors (points closest to the margin), SVM is less sensitive to outliers.
- **Kernel Trick**: SVM can handle non-linear relationships by using kernel functions, making it versatile for complex datasets.

**Real-world Applications of SVM**:
- **Image Classification**: Handwriting recognition, face detection.
- **Bioinformatics**: Classification of proteins and genes.
- **Text Categorization**: Spam detection, sentiment analysis.
- **Finance**: Credit risk analysis, stock price prediction.
- **Medical Diagnosis**: Cancer detection, disease classification.

**2. What is Naive Bayes classification? What are the advantages of Naive Bayes classifiers? [CO2]**

**Naive Bayes Classification**:
- Naive Bayes is a probabilistic classifier based on Bayes' theorem.
- It assumes independence between predictors, given the class label (naive assumption).





**Advantages of Naive Bayes**:
- **Simple and Fast**: Easy to implement and computationally efficient.
- **Scalability**: Works well with large datasets.
- **Effective with High-Dimensional Data**: Particularly suitable for text classification.
- **Less Training Data Required**: Performs well even with a smaller amount of training data.
- **Robust to Irrelevant Features**: The naive assumption of feature independence simplifies the computation.

**3. How is KNN different from k-means clustering? How does one measure the effectiveness of KNN? [CO2]**

**Differences between KNN and k-means clustering**:
- **KNN (K-Nearest Neighbors)**:
  - Supervised learning algorithm.
  - Used for classification and regression tasks.
  - Determines the class of a given sample based on the majority class of its k-nearest neighbors in the feature space.

- **k-means Clustering**:
  - Unsupervised learning algorithm.
  - Used for clustering tasks.
  - Partitions data into k clusters based on minimizing the variance within each cluster.

**Measuring the Effectiveness of KNN**:
- **Accuracy**: Proportion of correctly classified instances.
- **Confusion Matrix**: Provides insight into true positives, false positives, true negatives, and false negatives.
- **Precision and Recall**: Evaluate the model's performance in terms of relevance and completeness.
- **F1 Score**: Harmonic mean of precision and recall, providing a balance between the two.
- **ROC Curve and AUC**: Measure the model's ability to discriminate between classes.

**4. Explain the concept of continuous and discrete features in Naive Bayes classification. How does Naive Bayes handle different types of features? [CO2]**

**Continuous Features**:
- Continuous features can take any value within a range.
- Naive Bayes handles continuous features by assuming a distribution (typically Gaussian/normal distribution).
- **Gaussian Naive Bayes**: Assumes that the continuous features follow a normal distribution, and uses the probability density function to compute \( P(X|C) \).

**Discrete Features**:
- Discrete features take on a limited number of distinct values (e.g., categorical data).
- Naive Bayes handles discrete features using a multinomial or Bernoulli distribution.
- **Multinomial Naive Bayes**: Suitable for discrete data where features represent counts (e.g., word frequencies in text classification).
- **Bernoulli Naive Bayes**: Suitable for binary/boolean features, representing the presence or absence of a feature.

**Handling Different Types of Features**:
- **Mixed Feature Types**: Naive Bayes classifiers can handle both continuous and discrete features by applying appropriate probability distributions for each type.
- **Preprocessing**: Continuous features can be discretized, or appropriate distributions can be assumed.
- **Model Training**: During training, the classifier estimates the parameters (mean, variance for continuous; probabilities for discrete) for each feature given the class label.

In summary, Naive Bayes classifiers handle different types of features by making assumptions about their distributions, allowing them to calculate the necessary probabilities for classification effectively.