## Unit - 3

### 1. How is Logistic Regression Different from Linear Regression? Why Does Logistic Regression Come Under the Category of Classification Problems?

**Logistic Regression vs. Linear Regression:**

- **Purpose:**
  - **Linear Regression:** Used for predicting a continuous dependent variable based on one or more independent variables.
  - **Logistic Regression:** Used for predicting a categorical dependent variable, often binary (0 or 1), based on one or more independent variables.

- **Equation:**
  - **Linear Regression:** Predicts the output \( y \) using the equation \( y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n \).
  - **Logistic Regression:** Predicts the probability \( p \) of the binary outcome using the logistic function:
    \[ p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n)}} \]
    The predicted output is interpreted as the probability of the event occurring.

- **Output:**
  - **Linear Regression:** Outputs a continuous value.
  - **Logistic Regression:** Outputs a probability between 0 and 1, which can be thresholded to classify into binary outcomes (e.g., 0 or 1).

- **Loss Function:**
  - **Linear Regression:** Uses Mean Squared Error (MSE) as the loss function.
  - **Logistic Regression:** Uses Log-Loss (Binary Cross-Entropy) as the loss function.

**Why Logistic Regression is a Classification Problem:**
- **Prediction of Classes:** Logistic regression predicts the probability of a binary class, which can be converted into a discrete class label (0 or 1) by applying a threshold (e.g., 0.5).
- **Classification Task:** The primary goal is to classify observations into one of the two classes based on the predicted probabilities.

### 2. Differentiate Between Batch, Stochastic, and Mini-Batch Gradient Descent

**Gradient Descent:**
A method used to minimize the loss function in machine learning algorithms by iteratively adjusting the model parameters.

**Types of Gradient Descent:**

- **Batch Gradient Descent:**
  - **Definition:** Calculates the gradient using the entire training dataset.
  - **Advantages:** Stable convergence, smooth updates.
  - **Disadvantages:** Computationally expensive for large datasets, requires more memory.
  - **Usage:** Suitable for small to medium-sized datasets where computational efficiency is not a concern.

- **Stochastic Gradient Descent (SGD):**
  - **Definition:** Calculates the gradient using a single training example at each iteration.
  - **Advantages:** Faster updates, can escape local minima, less memory required.
  - **Disadvantages:** Noisy updates, may not converge as smoothly.
  - **Usage:** Suitable for large datasets and online learning scenarios where data arrives in a stream.

- **Mini-Batch Gradient Descent:**
  - **Definition:** Calculates the gradient using a small batch of training examples at each iteration.
  - **Advantages:** Balances between batch and stochastic gradient descent, faster convergence, less noise in updates.
  - **Disadvantages:** Requires tuning of batch size, computational overhead.
  - **Usage:** Often used in practice for deep learning and large-scale datasets.

### 3. What is Linear Regression, and How Does It Model the Relationship Between Variables?

**Linear Regression:**
A statistical method used to model the relationship between a dependent variable \( y \) and one or more independent variables \( x \).

**Model:**
\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon \]
Where:
- \( y \) is the dependent variable.
- \( \beta_0 \) is the intercept.
- \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients for the independent variables \( x_1, x_2, \ldots, x_n \).
- \( \epsilon \) is the error term.

**How It Models Relationships:**
- **Fitting a Line:** Linear regression fits a straight line (or hyperplane in higher dimensions) to the data points.
- **Minimizing Errors:** It minimizes the sum of the squared differences between the observed values and the predicted values (Least Squares method).
- **Interpretation:** The coefficients \( \beta \) represent the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.

### 4. Explain the Concept of Polynomial Regression and When It Is Used

**Polynomial Regression:**
An extension of linear regression where the relationship between the independent variable \( x \) and the dependent variable \( y \) is modeled as an \( n \)-degree polynomial.

**Model:**
\[ y = \beta_0 + \beta_1 x + \beta_2 x^2 + \ldots + \beta_n x^n + \epsilon \]
Where:
- \( y \) is the dependent variable.
- \( \beta_0 \) is the intercept.
- \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients for the polynomial terms \( x, x^2, \ldots, x^n \).
- \( \epsilon \) is the error term.

**When It Is Used:**
- **Non-Linear Relationships:** When the relationship between the independent and dependent variables is non-linear.
- **Better Fit:** Provides a better fit for data that shows curvature.
- **Complex Models:** Useful in scenarios where the relationship cannot be captured by a straight line but can be approximated by a polynomial.

**Example:**
In a dataset showing the growth of plants over time, a quadratic or cubic polynomial might better capture the growth pattern than a simple linear model.

### 5. What Are Categorical Variables, and Why Are They Important in Regression Analysis?

**Categorical Variables:**
Variables that represent categories or groups and do not have a natural order or numerical value. Examples include gender, color, brand, etc.

**Types:**
- **Nominal:** No natural order (e.g., colors: red, green, blue).
- **Ordinal:** Natural order but not numerical (e.g., ratings: poor, fair, good, excellent).

**Importance in Regression Analysis:**
- **Modeling Real-World Data:** Many real-world datasets include categorical data that must be incorporated into regression models.
- **Feature Representation:** Categorical variables often contain significant information that can improve model performance.
- **Dummy Variables:** Categorical variables are typically converted into dummy variables (one-hot encoding) to be used in regression models.

**Example:**
In a housing price prediction model, the "neighborhood" could be a significant categorical variable, where different neighborhoods have different impacts on house prices.

### 6. What is Lasso Regression, and How Does It Differ from Ridge Regression?

**Lasso Regression (Least Absolute Shrinkage and Selection Operator):**
A type of linear regression that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model.

**Formula:**
\[ \beta^{lasso} = \underset{\beta}{\operatorname{argmin}} \left\{ \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \beta)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\} \]
Where:
- \( \lambda \) is the regularization parameter.
- \( \beta \) are the regression coefficients.

**Key Features:**
- **L1 Penalty:** Lasso uses the L1 penalty (\( \sum_{j=1}^p |\beta_j| \)), which can shrink some coefficients to exactly zero, effectively performing variable selection.
- **Sparse Solutions:** Encourages sparse solutions where only a subset of features are retained, leading to simpler and more interpretable models.

**Differences from Ridge Regression:**
- **Penalty Term:**
  - **Lasso:** Uses the L1 penalty, which can set some coefficients to zero.
  - **Ridge:** Uses the L2 penalty (\( \sum_{j=1}^p \beta_j^2 \)), which shrinks coefficients but does not set them to zero.
- **Variable Selection:**
  - **Lasso:** Can perform variable selection by shrinking some coefficients to zero.
  - **Ridge:** Does not perform variable selection, as it only shrinks coefficients.
- **Use Cases:**
  - **Lasso:** Preferred when we believe only a subset of features are important (sparse solutions).
  - **Ridge:** Preferred when all features are expected to contribute to the outcome but need regularization to prevent overfitting.

**Example:**
In a model predicting customer churn, Lasso regression might reduce the number of features by selecting only the most relevant customer attributes, while Ridge regression would shrink all attribute coefficients but retain all features.


## 8. Differences Between Simple Linear Regression and Multiple Linear Regression

### Simple Linear Regression
**Definition:**
Simple linear regression models the relationship between two variables by fitting a linear equation to observed data. It predicts the dependent variable (response) using a single independent variable (predictor).

**Equation:**
\[ y = \beta_0 + \beta_1 x \]
Where:
- \( y \) is the dependent variable.
- \( x \) is the independent variable.
- \( \beta_0 \) is the y-intercept.
- \( \beta_1 \) is the slope of the line.

**Example:**
Predicting a person's weight based on their height.

### Multiple Linear Regression
**Definition:**
Multiple linear regression models the relationship between one dependent variable and two or more independent variables.

**Equation:**
\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n \]
Where:
- \( y \) is the dependent variable.
- \( x_1, x_2, \ldots, x_n \) are the independent variables.
- \( \beta_0 \) is the y-intercept.
- \( \beta_n \) are the coefficients for each independent variable.

**Example:**
Predicting a person's weight based on their height, age, and gender.

## 9. Define Polynomial Regression and Explain How It Extends Linear Regression to Capture Non-Linear Relationships

**Polynomial Regression:**
Polynomial regression is an extension of linear regression where the relationship between the independent variable \( x \) and the dependent variable \( y \) is modeled as an nth degree polynomial.

**Equation:**
\[ y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_n x^n \]

**Extension of Linear Regression:**
- Polynomial regression extends linear regression by introducing non-linear terms of the independent variable.
- By including higher-order terms (squared, cubed, etc.), polynomial regression can model more complex, non-linear relationships.

**Example:**
Predicting the growth rate of a plant over time where the growth rate accelerates or decelerates over time.

## 10. Define Elastic Net Regression and Discuss Its Advantages Over Ridge and Lasso Regression

**Elastic Net Regression:**
Elastic net regression is a regularization technique that linearly combines the penalties of both the Lasso and Ridge regression methods.

**Equation:**
\[ \text{Elastic Net Loss} = \text{RSS} + \alpha \left( \frac{1 - \lambda}{2} \sum_{j=1}^n \beta_j^2 + \lambda \sum_{j=1}^n |\beta_j| \right) \]
Where:
- \( \text{RSS} \) is the residual sum of squares.
- \( \alpha \) is the overall regularization parameter.
- \( \lambda \) is the mixing parameter between Ridge (\( \lambda = 0 \)) and Lasso (\( \lambda = 1 \)).

**Advantages Over Ridge and Lasso:**
- **Feature Selection and Shrinkage:** Like Lasso, Elastic Net can perform variable selection and shrinkage simultaneously.
- **Handling Multicollinearity:** Elastic Net is useful when the predictors are highly correlated. It combines the benefits of Ridge regression (handling multicollinearity) and Lasso (variable selection).

## 11. Define Logistic Regression and Discuss Its Applications in Binary and Multinomial Classification Problems

**Logistic Regression:**
Logistic regression is a statistical method used for binary classification that models the probability that a given input point belongs to a particular class.

**Equation:**
\[ P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_n x_n)}} \]
Where:
- \( P(y=1|x) \) is the probability of the class label being 1 given the input features \( x \).
- \( \beta_0 \) is the intercept.
- \( \beta_n \) are the coefficients for each independent variable.

**Applications:**
- **Binary Classification:** Predicting whether an email is spam or not spam.
- **Multinomial Classification:** Extending logistic regression to handle multiple classes (e.g., predicting the category of an article: sports, politics, or technology).

## 12. (a) Why is Logistic Regression Called Regression and Not Classification?

**Reason:**
Logistic regression is called "regression" because it predicts the probability of the outcome using a logistic function. It models the relationship between a dependent variable (binary outcome) and one or more independent variables through a regression equation. Despite being used for classification tasks, the technique itself involves fitting a regression model to predict probabilities.

## 12. (b) What are the Disadvantages of Logistic Regression?

**Disadvantages:**
- **Linear Boundaries:** Assumes a linear relationship between the independent variables and the log-odds of the dependent variable.
- **Not Effective with Non-Linear Data:** Less effective for complex, non-linear relationships unless feature engineering is performed.
- **Requires Large Sample Size:** Performance can degrade with small datasets.
- **Sensitive to Outliers:** Logistic regression can be sensitive to outliers, which can skew the results.
- **Assumption of Independence:** Assumes that the observations are independent of each other.

## 13. (a) Discuss About 5 Assumptions of Linear Regression

**Assumptions:**
1. **Linearity:** The relationship between the dependent and independent variables should be linear.
2. **Independence:** Observations should be independent of each other.
3. **Homoscedasticity:** The residuals (errors) should have constant variance at all levels of the independent variables.
4. **Normality of Residuals:** The residuals should be approximately normally distributed.
5. **No Multicollinearity:** Independent variables should not be highly correlated with each other.

## 13. (b) What is the Method of Maximum Likelihood?

**Maximum Likelihood Estimation (MLE):**
MLE is a method used to estimate the parameters of a statistical model by maximizing the likelihood function. The likelihood function measures how likely it is that the observed data would occur given a set of parameter values.

**Steps:**
1. **Define Likelihood Function:** Based on the probability distribution of the data.
2. **Maximize Likelihood:** Find the parameter values that maximize the likelihood function.
3. **Optimization:** Use optimization techniques (e.g., gradient descent) to find the maximum likelihood estimates.

## 14. How is Regression Different from Classification?

**Regression:**
- **Goal:** Predict a continuous output variable.
- **Output:** Numerical value (e.g., predicting house prices).
- **Examples:** Linear regression, polynomial regression.

**Classification:**
- **Goal:** Predict a categorical output variable.
- **Output:** Class label (e.g., predicting whether an email is spam or not).
- **Examples:** Logistic regression, decision trees, SVM.

## Property Rent and Area Linear Regression Example

### Given Data
| Area (ftÂ²) | Rent (Dollars) |
|------------|----------------|
| 340        | 500            |
| 510        | 700            |
| 640        | 900            |
| 880        | 1100           |
| 1080       | 1300           |
| 1700       | 1500           |

### Finding the Relationship Using Linear Regression

1. **Plot the Data:**
   Plot the given data points on a scatter plot to visualize the relationship between area and rent.

2. **Fit the Linear Regression Model:**
   Using the least squares method, fit a linear regression line to the data.

3. **Calculate the Slope and Intercept:**
   \[
   y = \beta_0 + \beta_1 x
   \]

   - \(\beta_0\): y-intercept.
   - \(\beta_1\): slope.

   Use the following formulas:
   \[
   \beta_1 = \frac{N \sum{xy} - \sum{x} \sum{y}}{N \sum{x^2} - (\sum{x})^2}
   \]
   \[
   \beta_0 = \frac{\sum{y} - \beta_1 \sum{x}}{N}
   \]

3. **Calculate the Slope and Intercept Using Given Data:**

   Let's calculate the sum values:
   \[
   \sum{x} = 340 + 510 + 640 + 880 + 1080 + 1700 = 5150
   \]
   \[
   \sum{y} = 500 + 700 + 900 + 1100 + 1300 + 1500 = 6000
   \]
   \[
   \sum{xy} = (340 \cdot 500) + (510 \cdot 700) + (640 \cdot 900) + (880 \cdot 1100) + (1080 \cdot 1300) + (1700 \cdot 1500) = 869000
   \]
   \[
   \sum{x^2} = 340^2 + 510^2 + 640^2 + 880^2 + 1080^2 + 1700^2 = 5213600
   \]

   Number of data points \( N = 6 \).

   \[
   \beta_1 = \frac{6 \cdot 869000 - 5150 \cdot 6000}{6 \