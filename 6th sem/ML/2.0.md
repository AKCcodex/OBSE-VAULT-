## UNIT - 2

### 1. How does the correlation coefficient determine feature relevance?
The correlation coefficient measures the linear relationship between two variables. In feature selection, the correlation coefficient helps determine the relevance of a feature by assessing how strongly it is related to the target variable. 

**Key Points:**
- **Range:** The correlation coefficient, \( r \), ranges from -1 to 1.
  - \( r = 1 \) indicates a perfect positive linear relationship.
  - \( r = -1 \) indicates a perfect negative linear relationship.
  - \( r = 0 \) indicates no linear relationship.
- **Feature Relevance:** A high absolute value of \( r \) (close to 1 or -1) suggests that the feature is relevant and strongly associated with the target variable.
- **Caveats:** Correlation does not imply causation, and it only captures linear relationships. Non-linear relationships require other methods.

### 2. Explain true positives, true negatives, false positives, and false negatives in a confusion matrix.
A confusion matrix is a table used to evaluate the performance of a classification model. It compares the actual target values with the model's predictions.

**Components:**
- **True Positives (TP):** Instances where the model correctly predicts the positive class.
- **True Negatives (TN):** Instances where the model correctly predicts the negative class.
- **False Positives (FP):** Instances where the model incorrectly predicts the positive class (Type I error).
- **False Negatives (FN):** Instances where the model incorrectly predicts the negative class (Type II error).

**Example:**
For a binary classification problem with classes "Positive" and "Negative":

|                | Predicted Positive | Predicted Negative |
|----------------|--------------------|--------------------|
| Actual Positive | TP                 | FN                 |
| Actual Negative | FP                 | TN                 |

### 3. What are residuals, and why are they important in regression?
**Residuals:** The difference between the observed values and the predicted values of a regression model.

**Formula:** \(\text{Residual} = y_i - \hat{y}_i\), where \( y_i \) is the observed value and \( \hat{y}_i \) is the predicted value.

**Importance:**
- **Model Accuracy:** Residuals help in assessing the accuracy of a regression model.
- **Model Fit:** Analyzing residuals can indicate how well the model captures the data's underlying patterns.
- **Assumptions:** Residuals should be randomly distributed; patterns in residuals can indicate model misspecification, heteroscedasticity, or the presence of outliers.

### 4. Define a confusion matrix in classification problems.
A confusion matrix is a table used to describe the performance of a classification algorithm. It shows the actual versus predicted classifications and helps in calculating various evaluation metrics.

**Structure:**

|                | Predicted Positive | Predicted Negative |
|----------------|--------------------|--------------------|
| Actual Positive | TP                 | FN                 |
| Actual Negative | FP                 | TN                 |

**Metrics Derived:**
- **Accuracy:** \((TP + TN) / (TP + TN + FP + FN)\)
- **Precision:** \(TP / (TP + FP)\)
- **Recall (Sensitivity):** \(TP / (TP + FN)\)
- **F1 Score:** \(2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\)

### 5. How are dependent and independent variables related in regression?
- **Dependent Variable (Response Variable):** The variable we aim to predict or explain.
- **Independent Variable (Predictor Variable):** The variable(s) used to make predictions about the dependent variable.

**Relationship in Regression:**
- **Linear Regression:** Assumes a linear relationship between the dependent variable (\(Y\)) and one or more independent variables (\(X\)).
  \[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n + \epsilon \]
  Where:
  - \( \beta_0 \) is the intercept,
  - \( \beta_1, \beta_2, \ldots, \beta_n \) are the coefficients,
  - \( \epsilon \) is the error term.

### 6. How is the Chi-Square test applied in feature selection?
The Chi-Square test assesses the independence of two categorical variables. In feature selection, it is used to determine if there is a significant association between a feature and the target variable.

**Steps:**
1. **Null Hypothesis (\(H_0\)):** Assumes that the feature and target variable are independent.
2. **Calculate Chi-Square Statistic:** 
   \[ \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \]
   Where \(O_i\) is the observed frequency and \(E_i\) is the expected frequency.
3. **Compare with Critical Value:** Determine the p-value and compare it with a significance level (e.g., 0.05). If the p-value is less than the significance level, reject the null hypothesis.
4. **Feature Relevance:** A low p-value indicates that the feature is significantly associated with the target variable, suggesting its relevance.

### 7. What does the term "principal components" refer to in PCA?
**Principal Components:** In Principal Component Analysis (PCA), principal components are new variables constructed as linear combinations of the original variables. They are orthogonal and capture the maximum variance in the data.

**Key Points:**
- **First Principal Component:** Captures the largest variance.
- **Subsequent Components:** Capture the remaining variance in descending order.
- **Dimensionality Reduction:** Principal components can be used to reduce the dimensionality of the data while retaining most of the variance.

### 8. How are mean deviation, standard deviation, and variance related to each other?
- **Mean Deviation (Mean Absolute Deviation):**
  - **Definition:** The average of the absolute deviations from the mean.
  - **Formula:** \(\text{Mean Deviation} = \frac{1}{n} \sum |x_i - \bar{x}|\)

- **Variance:**
  - **Definition:** The average of the squared deviations from the mean.
  - **Formula:** \(\text{Variance} ( \sigma^2 ) = \frac{1}{n} \sum (x_i - \bar{x})^2\)

- **Standard Deviation:**
  - **Definition:** The square root of the variance, representing the average deviation from the mean.
  - **Formula:** \(\text{Standard Deviation} ( \sigma ) = \sqrt{\frac{1}{n} \sum (x_i - \bar{x})^2}\)

**Relationship:**
- **Variance to Standard Deviation:** \(\text{Standard Deviation} = \sqrt{\text{Variance}}\)
- **Mean Deviation vs. Standard Deviation:** Mean deviation is less sensitive to outliers compared to standard deviation, but both measure the dispersion of data around the mean. Variance and standard deviation take squared deviations into account, giving more weight to larger deviations.

## 1. Describe the Chi-Square Test and its Application in Feature Selection

### Chi-Square Test:
The Chi-Square test is a statistical method used to determine if there is a significant association between two categorical variables. It compares the observed frequencies of events to the expected frequencies under the assumption of independence.

**Key Points:**
- **Null Hypothesis (\(H_0\)):** Assumes that the two variables are independent.
- **Alternative Hypothesis (\(H_A\)):** Assumes that the two variables are not independent.

**Formula:**
\[ \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i} \]
Where:
- \(O_i\) is the observed frequency.
- \(E_i\) is the expected frequency, calculated as \(E_i = \frac{\text{row total} \times \text{column total}}{\text{grand total}}\).

**Steps:**
1. **Calculate Expected Frequencies:** Based on the marginal totals of the contingency table.
2. **Compute Chi-Square Statistic:** Using the formula above.
3. **Determine Degrees of Freedom:** \(df = (r - 1)(c - 1)\), where \(r\) is the number of rows and \(c\) is the number of columns.
4. **Compare with Critical Value:** Using the Chi-Square distribution table. Alternatively, calculate the p-value.
5. **Decision:** Reject the null hypothesis if the p-value is less than the significance level (e.g., 0.05).

### Application in Feature Selection:
In feature selection, the Chi-Square test is used to identify the relevance of categorical features in relation to the target variable. 

**Process:**
1. **Formulate Hypotheses:**
   - \(H_0\): The feature is independent of the target variable.
   - \(H_A\): The feature is dependent on the target variable.
2. **Calculate Chi-Square Statistic:** For each feature-target pair.
3. **Select Features:** Features with a low p-value (below a significance threshold) are considered relevant and selected for the model.

**Example:**
Consider a dataset with a binary target variable (e.g., "Buy" or "Not Buy") and a categorical feature (e.g., "Age Group" with categories "Young", "Middle-aged", "Senior"). The Chi-Square test can be used to determine if the "Age Group" feature is significantly associated with the purchasing behavior.

## 2. Examples of Real-World Applications Where Wrapper Methods are Beneficial for Feature Selection

### Wrapper Methods:
Wrapper methods evaluate feature subsets based on their performance with a specific machine learning model. They involve training and testing the model with different feature combinations and selecting the subset that yields the best performance.

### Real-World Applications:

1. **Medical Diagnosis:**
   - **Problem:** Selecting the most relevant biomarkers for disease prediction from a large set of potential biomarkers.
   - **Wrapper Method Benefit:** Provides a more accurate and relevant subset of features, leading to better diagnostic performance.

2. **Customer Churn Prediction:**
   - **Problem:** Identifying the key factors that influence customer churn from a vast array of customer data attributes.
   - **Wrapper Method Benefit:** Improves model accuracy by focusing on the most impactful features, helping businesses to take targeted actions to reduce churn.

3. **Text Classification:**
   - **Problem:** Selecting the most informative words or phrases from a large corpus of text for sentiment analysis or topic classification.
   - **Wrapper Method Benefit:** Enhances model performance by identifying the most relevant textual features, improving classification accuracy.

4. **Financial Risk Assessment:**
   - **Problem:** Identifying the key indicators of financial risk from a multitude of financial metrics and market data.
   - **Wrapper Method Benefit:** Yields a more robust and accurate risk prediction model by selecting the most significant financial indicators.

5. **Fraud Detection:**
   - **Problem:** Detecting fraudulent transactions based on various transaction features and user behavior data.
   - **Wrapper Method Benefit:** Improves detection accuracy by focusing on the most indicative features of fraudulent activity.

## 3. Advantages and Disadvantages of Using Decision Trees for Feature Selection

### Advantages:

1. **Interpretability:**
   - Decision trees provide a clear and visual representation of feature importance and decision-making processes, making them easy to interpret and understand.

2. **Non-Parametric:**
   - Decision trees do not assume any specific distribution for the data, making them suitable for a wide range of data types and distributions.

3. **Handling Non-Linear Relationships:**
   - Decision trees can capture complex non-linear relationships between features and the target variable.

4. **Automatic Feature Selection:**
   - During the tree-building process, decision trees inherently perform feature selection by choosing the most informative features for splitting nodes.

5. **Robustness to Irrelevant Features:**
   - Decision trees are relatively robust to irrelevant features, as they tend to ignore features that do not contribute to improving the model's performance.

### Disadvantages:

1. **Overfitting:**
   - Decision trees are prone to overfitting, especially with small datasets or trees with many deep branches. This can lead to poor generalization on unseen data.

2. **Instability:**
   - Small changes in the data can lead to significant changes in the structure of the tree, making decision trees unstable.

3. **Bias Toward Features with More Levels:**
   - Decision trees can be biased towards features with more levels or categories, as these features can provide more opportunities for splitting the data.

4. **Limited to Axis-Parallel Splits:**
   - Decision trees make axis-parallel splits, which can be suboptimal for certain datasets where diagonal or complex decision boundaries are more appropriate.

5. **Complexity with Large Trees:**
   - As the number of features increases, the complexity of the decision tree can grow exponentially, making it computationally expensive and harder to interpret.

## 4. How Ridge Regression Introduces Regularization and Comparison with Ordinary Least Squares Regression

### Ridge Regression:

**Definition:**
Ridge regression, also known as Tikhonov regularization, is a technique that introduces a regularization term to the ordinary least squares (OLS) regression to prevent overfitting by penalizing large coefficients.

**Formula:**
\[ \beta^{ridge} = \underset{\beta}{\operatorname{argmin}} \left\{ \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\} \]
Where:
- \( \lambda \) is the regularization parameter.
- \( \beta \) are the regression coefficients.

### How Regularization is Introduced:
- **Penalty Term:** The term \( \lambda \sum_{j=1}^p \beta_j^2 \) adds a penalty proportional to the sum of the squared values of the coefficients.
- **Shrinkage:** The regularization parameter \( \lambda \) controls the amount of shrinkage applied to the coefficients. As \( \lambda \) increases, the coefficients are shrunk more towards zero.
- **Bias-Variance Tradeoff:** Regularization introduces a bias but reduces variance, helping to improve the model's generalization ability.

### Comparison with Ordinary Least Squares (OLS) Regression:

**Ordinary Least Squares Regression:**
- **Objective:** Minimize the sum of the squared residuals.
- **Formula:**
  \[ \beta^{OLS} = \underset{\beta}{\operatorname{argmin}} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \beta)^2 \]

**Differences:**
1. **Overfitting:**
   - **OLS:** Prone to overfitting, especially when the number of features is large or when multicollinearity exists.
   - **Ridge Regression:** Reduces overfitting by penalizing large coefficients and shrinking them towards zero.

2. **Complexity:**
   - **OLS:** Coefficients can be large and unstable when features are highly correlated.
   - **Ridge Regression:** Provides more stable and interpretable coefficients by imposing a constraint on their size.

3. **Bias-Variance Tradeoff:**
   - **OLS:** Low bias but high variance, which can lead to poor generalization on new data.
   - **Ridge Regression:** Introduces bias through regularization but reduces variance, improving generalization.

4. **Handling Multicollinearity:**
   - **OLS:** Multicollinearity can cause large variances in the estimated coefficients.
   - **Ridge Regression:** Effectively handles multicollinearity by shrinking correlated coefficients towards each other.

5. **Solution Uniqueness:**
   - **OLS:** May not have a unique solution when the number of features exceeds the number of observations.
   - **Ridge Regression:** Always has a unique solution due to the regularization term.

### Conclusion:
Ridge regression is a powerful technique to enhance the generalization ability of regression models by introducing a regularization term. It effectively addresses issues like overfitting and multicollinearity that are common in ordinary least squares regression, making it a valuable tool in the arsenal of statistical modeling.